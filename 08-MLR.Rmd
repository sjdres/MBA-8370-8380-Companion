# Multiple Linear Regression {#MLR}

```{r, include=FALSE}
knitr::opts_chunk$set(fig.align='center', message = FALSE)
library(pander)
library(plot3D)
```

*Sometimes one independent variable just doesn't cut it.*

$$PRF:\;Y_i=\beta_0+\beta_1X_{1i}+\beta_2X_{2i}+...+\beta_kX_{ki}+\varepsilon_i$$

$$SRF:\;Y_i=\hat{\beta}_0+\hat{\beta}_1X_{1i}+\hat{\beta}_2X_{2i}+...+\hat{\beta}_kX_{ki}+e_i$$

A **Multiple Regression Model** is a direct extension of the **Simple Regression Model** by adding additional independent variables. Adding additional independent variables allows the regression to use more information when trying to explain movements in the single dependent variable. In other words, multiple independent variables can explain changes in the dependent variable along different *dimensions*.

The multiple regression model has a lot in common with the simple regression model.

1. It is still the case that we establish a *population regression function* (PRF) that we believe holds in the population, but we are forced to estimate a *sample regression function* (SRF) because we can only observe a sample (i.e., subset) of the population.

2. The SRF is still solved via OLS. The first-order conditions are a bit more complicated than those stemming from a simple regression, but are conceptually the same.

3. The PRF and SRF still each contain a single intercept term and a single residual term.

4. The model we are examining is still a *line* equation - only it is a multi-dimensional line equation (i.e., a plane in the case of two dimensions).

The only significant change we need to make is with respect to the interpretation of the slope coefficients of our model. These slope coefficients still deliver the *expected or average change in the dependent variable given a unit change in an independent variable*. However, since we are looking at multiple independent variables simultaneously, we need to be **explicit** that we are examining these relationships *one independent variable at a time*. In other words, when we examine the relationship between the dependent variable and a particular independent variable, we need to explicitly state that we are holding all other independent variables *constant*.

$$\beta_k=\frac{\Delta Y}{\Delta X_{k}}$$

>In the population: a PRF slope coefficient indicates the EXPECTED or AVERAGE change in the dependent variable associated with a one-unit increase in the kth explanatory variable holding all other explanatory variables constant.

$$\hat{\beta}_k=\frac{\Delta Y}{\Delta X_{k}}$$

>In the sample: a SRF slope coefficient indicates the expected or average change in the dependent variable associated with a one-unit increase in the kth explanatory variable holding all other explanatory variables constant.

## Application: Explaining house price in a multiple regression

Let us revisit the relationship between house price and house size, but extend the regression model to include a second independent variable: the number of bedrooms.

Our PRF becomes:

$$price_i=\beta_0+\beta_1 \;sqrft_i+\beta_2 \;bdrms_i+\varepsilon_i$$

Our SRF becomes:

$$price_i=\hat{\beta}_0+\hat{\beta}_1 \;sqrft_i+\hat{\beta}_2 \;bdrms_i+e_i$$

To visualize what we are about to do, lets start with scatter plots looking at the relationships between the dependent variable and each independent variable.

The figure on the left is the scatter plot between the House Price and House Size. This positive relationship is exactly what we have looked at previously. The figure on the right is the scatter plot between the same House Price but the number of bedrooms each house has. This figure illustrates that the houses in our sample have between 2 and 7 bedrooms (with no half-rooms), and homes with more bedrooms generally have higher prices (as expected). Note that we are looking at the same dependent variable along different *dimensions*. We can combine these dimensions into a singe (3-Dimensional) figure to see how the relationships between the dependent variable and each independent variable appear simultaneously. 

```{r}
data(hprice1,package='wooldridge')
Y <- hprice1$price
X1 <- hprice1$sqrft
X2 <- hprice1$bdrms

par(mfrow = c(1,2))
plot(X1,Y, col = "blue",
          pch = 19, cex = 1,
          xlab = "House Size", ylab = "House Price")
plot(X2,Y, col = "red",
          pch = 19, cex = 1,
          xlab = "Bedrooms", ylab = "House Price")
```

```{r, fig.width=6.5, fig.height=6.5}
scatter3D(X1, X2, Y, pch = 20, cex = 1, phi = 0,
          colkey=FALSE, ticktype = "detailed",
          xlab = "House Size", ylab = "Bedrooms",
          zlab = "House Price", main = "3D Scatterplot")
```

For comparison, suppose that we consider these independent variables one at a time in two simple regressions. In particular, we can examine one simple regression model where House Size is the only independent variable, and another simple regression model where Bedrooms is the only independent variable.^[Note that this is only for explanatory purposes only. If this were an actual analysis, we really wouldn't gain much from considering each independent variable separately.]

```{r}
REG1 <- lm(hprice1$price ~ hprice1$sqrft)
coef(REG1)

REG2 <- lm(hprice1$price ~ hprice1$bdrms)
coef(REG2)
```

The regression considering house size alone has a slope coefficient of 0.14. Remember that since house price was denoted in thousands of dollars and house size was denoted in square feet, this slope coefficient states that an additional square foot of house size will increase the average house price by \$140. 

The regression considering number of bedrooms alone has a slope coefficient of 62. This slope coefficient states that an additional bedroom will increase the average house price by \$62,000.

While the results from these two simple regressions make sense, we need to realize that a simple regression model only considers a single independent variable (and throws all of the other information into the garbage can). This means the first regression takes no notice of the number of rooms a house has, while the second regression takes no notice of the size of the home. Since it is reasonable to assume that bigger homes have more bedrooms, then a regression model that is only given one of these pieces of information might be overstating the quantitative impact of the single independent variable.

To illustrate this, let us run a multiple regression model where both house size and number of bedrooms are considered.

```{r}
REG3 <- lm(price ~ sqrft + bdrms, data = hprice1)
coef(REG3)
```

The slope with respect to house size is now 0.128 (down from 0.14) while the slope with respect to number of bedrooms is now 15.2 (down from 62). In order to make sense of these changes, let us explicitly interpret these slope coefficients within the context of a multiple regression model (where we can hold all other independent variables constant).

>Holding number of bedrooms constant, an additional square foot of house size will increase a house price by \$128, on average.

>Holding house size constant, an additional bedroom will increase a house price by \$15,200, on average.

The power of a multiple regression comes through when you look at the second slope interpretation. Multiple regression allows us to consider two houses that have the same house size but one house has an additional bedroom. In other words, imagine building a wall that turns one bedroom into two smaller bedrooms (and doesn't change the house size). This will increase the expected house price by \$15,200. This is much smaller than the simple regression relationship of \$62,000 because the simple regression could not differentiate the impact of a bedroom from the impact of an increase in house size. The multiple regression model can.

The final figure shows the 3-dimensional regression line (i.e., plane) that best fits the sample. You can see that considering multiple dimensions increases the performance of the deterministic component of the model and therefore reduces the amount of information that goes into the garbage can as *unpredictable*. This can be shown in the last picture that only looks at the relationship from the "sqrft" dimension. In the figure on the left, the blue dots are the observations in the data, the black line is the regression line from the simple regression model (without Bedrooms), while the red dots are the model predictions from the multiple regression model with Bedrooms included as an additional independent variable. Notice how this allows the regression predictions to veer off of a straight line. This results in slightly less prediction errors showing up in your garbage can - as illustrated in the figure on the right.

```{r, echo = FALSE, fig.width=6.5, fig.height=6.5}

fit <- lm(Y ~ X1 + X2)

# predict values on regular xy grid
grid.lines = 4
x.pred <- seq(min(X1), max(X1), length.out = grid.lines)
y.pred <- c(min(X2), max(X2))
xy <- expand.grid( X1 = x.pred, X2 = y.pred)
z.pred <- matrix(predict(fit, newdata = xy), 
                 nrow = grid.lines, ncol = 2)

# fitted points for droplines to surface
fitpoints <- predict(fit)
# scatter plot with regression plane
scatter3D(X1, X2, Y, pch = 19, cex = 1, 
    phi = 0, colkey = FALSE, ticktype = "detailed",
    xlab = "Size", ylab = "Rooms", zlab = "Price",  
    surf = list(x = x.pred, y = y.pred, z = z.pred,
    facets = NA, fit = fitpoints), main = "3D Regression Plane")

```

```{r}
par(mfrow = c(1,2))
plot(hprice1$sqrft,hprice1$price, col = "blue",
          pch = 19, cex = 1,
          xlab = "House Size", ylab = "House Price")
lines(hprice1$sqrft,fitted(REG1))
points(hprice1$sqrft,fitted(REG3),col = "red")

plot(hprice1$sqrft,residuals(REG1), col = "black",
          pch = 19, cex = 1,
          xlab = "House Size", ylab = "Residuals")
points(hprice1$sqrft,residuals(REG3),col = "red")
abline(h = 0,col="blue")
```

### The Importance of "Controls"

One very important item to point out in the last application is exactly why the coefficient on number of bedrooms dropped from \$62,000 to \$15,200 when the size of the house was added to the regression. The reason can be broken up into two categories.

#### 1. The independent variables are correlated {-}

It seems reasonable to believe that bigger houses have more bedrooms. This means that the size of a house and the number of bedrooms are correlated with each other. 

#### 2. "All Else Equal" in a Multiple Regression is more than just words {-}

A multiple regression can separately identify the average impact of each independent variable on the dependent variable. 

Put together, these two items suggest that when two independent variables are correlated, then they should both appear in the regression model. If not, then the correlation between an included independent variable and an omitted independent variable might lead to *omitted variable bias*. This is what we saw above in the regression with only number of bedrooms as an independent variable. The coefficient of $62,000 is giving you the combined impact of an additional room *and* a bigger house. When you add house size as another independent variable, you are now able to determine the expected increase in house price for an additional bedroom *holding house size constant*. 

Bottom line is that even though you are concerned with the results from a particular independent variable, it is important to try and include all independent variables that might be correlated with the independent variable of interest. This attempts to alleviate omitted variable bias.


## Adjusted $R^2$

Regardless of the number of independent variables, the variance of a regression model can be decomposed and a $R^2$ can be calculated.

$$TSS = \sum^{N}_{i=1}(Y_i - \bar{Y})^2$$

$$ESS = \sum^{N}_{i=1}(\hat{Y}_i - \bar{Y})^2$$

$$RSS = \sum^{N}_{i=1}(Y_i - \hat{Y}_i)^2 = \sum^{N}_{i=1}e_i^2$$

$$R^2 = \frac{ESS}{TSS} = 1 - \frac{RSS}{TSS}$$

The $R^2$ still delivers the proportion of the variation in the dependent variable explained by the model, only now the model is comprised of multiple independent variables. In other words, we are using more independent variables when comprising the deterministic component of our model and making expectation of the dependent variable.

$$\hat{Y}_i = \hat{\beta}_0+\hat{\beta}_1\;X_{1i}+\hat{\beta_2}\;X_{2i}+...+\hat{\beta_k}\;X_{ki}$$

An $R^2$ is a very intuitive calculation, but it sometimes might be misleading.

### Abusing an $R^2$

No matter how hard I try to downplay the importance of an $R^2$, students always have the tendency to shoot for that measure to be as close to 1 as possible. The problem with this goal is that an $R^2$ equal to 1 in not necessarily a good thing. Furthermore, achieving an $R^2$ of one might be *impossible*. 

An $R^2$ of 1 is sometimes impossible because our PRF actually has a residual term. This means that we understand that there is forecast error in the population. In other words, suppose that we are trying to understand the movements of a rather *noisy* dependent variable and 20 percent of its variation is entirely random and unpredictable. This would imply that a value of $R^2$ equal to $0.80$ is the **highest** value we can hope to achieve. If we somehow received a value higher than that... our sample regression function is not a valid representation of our population regression function.

Let us illustrate how an $R^2$ value can be misleading by way of an application. Consider a previous regression where we explained house prices with only the number of bedrooms.

```{r}
REG1 <- lm(price ~ bdrms, data = hprice1)
summary(REG1)$r.squared
```

The coefficient of determination states that the number of bedrooms explains slightly around `r round(summary(REG1)$r.squared,2)` percent of the variation in house prices. If we include the size of the house in the regression,

```{r}
REG2 <- lm(price ~ bdrms + sqrft, data = hprice1)
summary(REG2)$r.squared
```

we see that the $R^2$ increases to `r round(summary(REG2)$r.squared,2)` as before. If we include yet another variable such as the size of the property,

```{r}
REG3 <- lm(price ~ bdrms + sqrft + lotsize, data = hprice1)
summary(REG3)$r.squared
```

we see that the regression now explains about `r round(summary(REG3)$r.squared,2)` percent of the variation in house prices.

What we are seeing is that the more variables you add the higher the $R^2$ is getting. While this might lead you to believe that we are adding *important* independent variables to the regression, the problem is that the $R^2$ will go up **no matter what variable you add**. The increase might be slight, but the $R^2$ will never go down.

```{r}
Xcrap <- rnorm(88)
```


```{r}
REG4 <- lm(price ~ bdrms + sqrft + lotsize + Xcrap, data = hprice1)
summary(REG4)$r.squared
```

The exercise above adds a completely random variable as a fourth independent variable. It should have nothing to do with explaining house prices. However, if you generate *the correct* random variables, then you might get an increase in the $R^2$ by as much as an entire percentage point. Does this say that the random variable actually helps explain variations in house prices? Of course not. What it does show is that sometimes we can abuse the $R^2$, so we need an additional measure of goodness of fit.

### An *Adjusted* $R^2$

The problem with an $R^2$ is that it will increase no matter what independent variable you throw into the regression. If you think about it, if a regression with two independent variables explains 63 percent of the variation in the dependent variable, then adding a third variable (no matter how silly) will deliver a regression that will explain *no less* than 63 percent of the variation. We therefore cannot use the $R^2$ as an informal measure for whether or not we should include an independent variable because we don't know how *big* an increase in $R^2$ needs to be. We therefore need a goodness of fit measure that not only has the potential to increase when the added variable is deemed important, but has the potential to decrease when the variable is unimportant. This is called an *adjusted* $R^2$.

$$\bar{R}^2 = 1 - \frac{RSS/(N-k-1)}{TSS/(N-1)}$$

The main difference between the adjusted $R^2$ and it's unadjusted measure are the degrees of freedom in the numerator. When you add an additional independent variable, $k$ goes up by one but $N$ stays constant. Also, when adding an additional independent variable, the RSS goes down (which is what delivers an increase in the standard $R^2$). What you have in the numerator is a cost / benefit analysis. In other words, if the decrease in RSS is greater - then the $\bar{R}^2$ increases and the independent variable of question *might be somewhat important*. However, if the decrease in $N-k-1$ is greater, then the $\bar{R}^2$ decreases and the independent variable of question is *not important*.

#### Conclusion: for informal use only! {-}

While the $R^2$ and adjusted $R^2$ are two common measures of goodness of fit, they are informal at best. One can interpret them along the lines of how we did above, but there will more formal measures of whether or not an independent variable improves the forecasts of the regression model. Bottom line: these measures can give some insight to the results of a regression model, but they aren't anything worth hanging your final conclusions on.

## Statistical Inference

The course officially discusses statistical inference using the multiple regression model as opposed to the simple regression model, so this section should contain everything that is needed. If any preliminary material desired, there is an appendix to Chapter 7 that discusses statistical inference specifically with respect to the simple (one independent variable) regression model. One might also want to briefly review the chapters of statistical inference from MBA 8370 (i.e., Chapters 5 and 6).

### Recalling the Concept of Statistical Inference

Back in MBA 8370, we wanted to get an idea about the *parameters* of a *population* (i.e., the population mean $\mu$ and the population standard deviation $\sigma$), but only had concrete information on the *statistics* of a *sample* (i.e., the sample mean $\bar{X}$ and the sample standard deviation $S$). We were able to make probabilistic statements (i.e., *educated guesses*) concerning the population parameters given the sample statistics along the lines of *confidence intervals* and *hypothesis tests*. 

* Confidence Intervals allowed us to make general statements concerning the range of values in which the population mean $\mu$ will reside given the characteristics of the sample $(\bar{X},\;S)$ and a particular probability or *level of confidence* $\alpha$.

* Hypothesis Tests allowed us to determine if nonarbitrary statements concerning the value of the population mean $\mu$ are consistent or inconsistent with the characteristics of the sample $(\bar{X},\;S)$.

The same concept of statistical inference can be applied to regression models. A population regression model contains parameters such as the intercept, slope coefficients, and residual standard error $(\sigma_{XY})$.

$$PRF:\;Y_i=\beta_0+\beta_1X_{1i}+\beta_2X_{2i}+...+\beta_kX_{ki}+\varepsilon_i$$

We would like to know these population parameters, but we won't know them for sure because we cannot analyze the population. Since we can only observe a sample, we can estimate a sample regression model containing statistics such as the intercept, slope coefficients, and residual standard error $(S_{XY})$.

$$SRF:\;Y_i=\hat{\beta}_0+\hat{\beta}_1\;X_{1i}+\hat{\beta}_2\;X_{2i}+...+\hat{\beta}_k\;X_{ki}+e_i$$

Our statistical inference will again amount to using our sample characteristics to make probabilistic statements about our population parameters. Statistical inference will take the form of our familiar confidence intervals and hypothesis tests, and a new statistical inference tool of forecasting.

### Confidence Intervals (around population parameters)

Recall our earlier formula for calculating a confidence interval in a univariate context:

$$Pr\left(\bar{X}-t_{(\frac{\alpha}{2},df=n-1)}\frac{S}{\sqrt{n}} \leq \mu \leq \bar{X}+t_{(\frac{\alpha}{2},df=n-1)}\frac{S}{\sqrt{n}}\right)=1-\alpha$$

We used the Central Limit Theorem (CLT) to ultimately state that $\bar{X}$ was drawn from a normal distribution with a mean of $\mu$ and standard deviation $\sigma/\sqrt{n}$ (but we only have $S$ which makes this a t distribution). This line of reasoning is *very* similar to what we have with regression analyses.

First, $\hat{\beta}$ is an estimate of $\beta$ just like $\bar{X}$ is an estimate of $\mu$. However, the standard error of the sampling distribution of $\hat{\beta}$ is derived from the standard deviation of the residuals.

$$S_{\hat{\beta}}=\frac{S_{YX}}{\sum{(X_i-\bar{X})^2}}$$

with

$$S_{YX} = \sqrt{ \frac{\sum e_i^2}{n-k-1} }$$

This means that we construct a *standardized* random variable from a t distribution with $n-k-1$ degrees of freedom, where $k$ is the number of independent variables (or slope coefficients) in the regression model.^[Note that the appendix in Chapter 7 states that the t distribution in the simple regression case has $n-2$ degrees of freedom. This is because there is only one independent variable in a simple regression, so $k=1$ and $n-k-1 = n - 2$.]

$$t=\frac{\hat{\beta}-\beta}{S_{\hat{\beta}}}$$

We have already derived a confidence interval before, so we can skip to the punchline.

$$Pr\left(\hat{\beta}-t_{(\frac{\alpha}{2},df=n-k-1)}S_{\hat{\beta}} \leq \beta \leq \hat{\beta}+t_{(\frac{\alpha}{2},df=n-k-1)}S_{\hat{\beta}}\right)=1-\alpha$$

This is the formula for a confidence interval around the *population* slope coefficient $\beta$ given the estimate $\hat{\beta}$ and the regression characteristics. It can also be written compactly as before.

$$\hat{\beta} \pm t_{(\frac{\alpha}{2},df=n-k-1)} S_{\hat{\beta}}$$

Recall our regression explaining differences in house prices given information on house sizes and number of bedrooms.

```{r, eval = FALSE}
REG <- lm(price ~ sqrft + bdrms, data = hprice1)
summary(REG)
```
```{r, echo = FALSE}
REG <- lm(price ~ sqrft + bdrms, data = hprice1)
pander(summary(REG))
```

The information included in the regression summary is all that is needed for us to construct a 95 percent $(\alpha=0.05)$ confidence interval around the *population* slope coefficient $\beta_1$. In other words, we can build a range where the population slope between house price and size will reside with 95 percent confidence.

```{r}
# Back out all of the needed information:
Bhat1 <- summary(REG)$coef[2,1]
SBhat1 <- summary(REG)$coef[2,2]
n <- length(residuals(REG))
k = 2

# Find the critical t-distribution values... same as before
AL <- 0.05
df <- n-k-1
tcrit <- qt(AL/2,df,lower.tail = FALSE)

# Use the formula... same as before
(LEFT <- Bhat1 - tcrit * SBhat1)
(RIGHT <- Bhat1 + tcrit * SBhat1)
```

$$Pr(0.101 \leq \beta_1 \leq 0.156)=0.95$$

This states that while an increase in house size by one square foot will increase the house price by \$`r round(Bhat1,3)*1000` $(\hat{\beta_1})$ on average in the sample, we can also state that an increase in house size by one square foot will increase the house price on average *in the population* somewhere between \$`r round(LEFT,3)*1000` and \$`r round(RIGHT,3)*1000` with 95% confidence.

While the code above showed you how to calculate a confidence interval from scratch as we did before, there is an easier (one-line) way in R:

```{r}
confint(REG)
```


### Hypothesis Tests

We are able to conduct hypothesis tests regarding the values of the population regression coefficients. For example:

$$H_0:\beta_1 = 0 \quad vs. \quad H_1:\beta_1 \neq 0$$

In the context of our house price application, this null hypothesis states that the population slope between house price and size is zero... meaning that there is *no* relationship between the two variables in the population.

Given the null hypothesis above, we follow the remaining steps laid out previously: we calculate a test statistic under the null, calculate a p-value, and conclude.

The test statistic under the null is given by

$$t=\frac{\hat{\beta}_1 - \beta_1}{S_{\hat{\beta}_1}}$$

and this test statistic is drawn from a t distribution with $n-k-1$ degrees of freedom. Concluding this test is no more difficult that what we've done previously.

```{r}
B1 = 0
(tstat <- (Bhat1 - B1)/SBhat1)
(Pval <- pt(tstat,df,lower.tail=FALSE)*2)
(1-Pval)
```

Our results state that we can reject this null hypothesis with approximately 100% confidence, meaning that there is a statistically significant relationship between house prices and house sizes. By *statistically significant*, we are essentially saying that the population relationship is some number other than zero.

As with the confidence interval exercise above, we actually do not need to conduct hypothesis tests where the null sets the population parameter to zero because R does this automatically. If you look again at the columns to the right of the estimated coefficient $\hat{\beta}_1$ in the regression summary above, you will see a t value that is exactly what we calculated above and a p value that is essentially zero. This implies that a test with the null hypothesis set to zero is always done for you.

```{r}
summary(REG)
```

This isn't to say that *all* hypothesis tests are automatically done for you. 

Suppose a realtor believes that homes sell for \$150 per square foot. This is a non-arbitrary statement on a population parameter that delivers the following hypotheses, followed by a test statistic, p-value, and conclusion.

$$H_0:\beta_1=0.150 \quad vs. \quad H_1:\beta_1\neq0.150$$

```{r}
B1 = 0.150
(tstat <- (Bhat1 - B1)/SBhat1)
(Pval <- pt(tstat,df)*2)
(1-Pval)
```

Our p-value implies that there is a `r round(Pval,2)*100` percent chance of being wrong if we reject the null hypothesis. In other words, we can reject the null with at most `r round(1-Pval,2)*100` percent confidence. We therefore do not have evidence that the population slope is different from 0.150 with any traditional level of confidence (e.g., $\alpha \leq 0.10$).

One-sided tests are also like before. We can consider *right-tailed* tests where the rejection region (and p-value) are in the right tail, as well as *left-tailed* tests where the rejection region (and p-value) are in the left tail. Let us examine one of each. 

Suppose a realtor believes that homes sell for *more than* \$120 per square foot.^[Note that while the estimate from our sample is greater than 0.120, the statement we are testing is regarding what is going on in the population.] Since we can lend statistical support to this claim by rejecting everything else, This delivers the following hypotheses, which gives rise to a **right-tailed** test.

$$H_0:\beta_1\leq0.120 \quad vs. \quad H_1:\beta_1>0.120$$

We calculate a test statistic under the null as always. But since this is a right-tailed test, we calculate the p-value (and conclusion) by always calculating the area to the **right** of the test statistic.
```{r}
B1 = 0.120
(tstat <- (Bhat1 - B1)/SBhat1)
(Pval <- pt(tstat,df,lower.tail = FALSE))
(1-Pval)
```

Our test concludes that we can reject the null with at most `r round(1-Pval,2)*100` percent confidence.

Suppose a different realtor believes that homes sell for *less than* \$130 per square foot.^[Note that while the estimate from our sample is less than 0.130, the statement we are testing is regarding what is going on in the population.] Since we can lend statistical support to this claim by rejecting everything else, This delivers the following hypotheses, which gives rise to a **left-tailed** test.

$$H_0:\beta_1\geq0.130 \quad vs. \quad H_1:\beta_1<0.130$$

We calculate a test statistic under the null as always. But since this is a left-tailed test, we calculate the p-value (and conclusion) by always calculating the area to the **left** of the test statistic.
```{r}
B1 = 0.130
(tstat <- (Bhat1 - B1)/SBhat1)
(Pval <- pt(tstat,df,lower.tail = TRUE))
(1-Pval)
```

Our test concludes that we can reject the null with at most `r round(1-Pval,2)*100` percent confidence.





### Confidence Intervals (around forecasts)

A regression can also build confidence intervals around the conditional expectations (i.e., forecasts) of the dependent variable.

Suppose you want to use our model to predict the price of a 1000 square foot house with 3 bedrooms. The conditional expectation is calculated by using our regression coefficients, a value of house size of 1000, a value of bedrooms of 3, and setting our forecast error to zero.

$$\hat{Y}_i=\hat{\beta}_0+\hat{\beta}_1\;1000+\hat{\beta}_2\;3$$

```{r}
Bhat0 = summary(REG)$coef[1,1]
Bhat1 = summary(REG)$coef[2,1]
Bhat2 = summary(REG)$coef[3,1]

(Yhat = Bhat0 + Bhat1 * 1000 + Bhat2 * 3)
```

This calculation suggests that we expect a house with 1000 square feet and 3 bedrooms to sell for approximately `r round(Yhat,3)` (thousand) dollars. Another way to calculate this forecast is using the predict command in R. This command creates a new data frame that includes only the value for the independent variable you want to make a prediction with. The rest is done for you.

```{r}
predict(REG,data.frame(sqrft = 1000, bdrms = 3))
```

While this is an expected value based on the sample, we need to appreciate that we want to see what the prediction is in the population. We are able to build a confidence interval around this forecast in a number of ways.

* A confidence interval for the mean response

* A confidence interval for an individual response

#### The mean response: a confidence interval {-}

Suppose you want to build a confidence interval around the mean price for a 1000 square foot house with 3 bedrooms in the population. This is a conditional mean. In other words, we want the average house price but *only* for homes with a particular size. This conditional mean is generally given by $\mu_{Y|X}$ and in this case by $\mu_{Y|X_1=1000,\;X_2=3}$. Building a confidence interval for the mean response is given by

$$ \hat{Y}_{X} \pm t_{(\frac{\alpha}{2},df=n-k-1)}S_{YX} \sqrt{h_i}$$
or

$$ \hat{Y}_{X} - t_{(\frac{\alpha}{2},df=n-k-1)}S_{YX} \sqrt{h_i} \leq \mu_{Y|X} \leq \hat{Y}_{X} + t_{(\frac{\alpha}{2},df=n-k-1)}S_{YX} \sqrt{h_i}$$ 

where

* $\hat{Y}_{X}$ is the expectation of the dependent variable conditional on the desired value of $X$.

* $S_{YX}$ is the standard error of the estimate (calculated previously)

* $t_{(\frac{\alpha}{2},df=n-k-1)}$ is the critical t statistic for a given value of $\alpha$ (calculate previously)

* $h_i = \frac{1}{n}+\frac{(X_i - \bar{X})^2}{\sum_{i=1}^n(X_i - \bar{X})^2}$

This last variable $h_i$ is what is new to us and increases the size of the confidence interval when the desired value of $X_i$ is farther away from the average value of the observations $\bar{X}$. This variable can sometimes be difficult to calculate, but R again does it for you.^[Note that this equation is provided for only one independent variable. It becomes even more messy in a multivariate setting. However, the important concept is that this value gets larger when we consider values of X that are farther away from the average values in the sample.] In R, a confidence interval around the population mean is simply called a *confidence* interval.

```{r}
predict(REG,
        data.frame(sqrft = 1000,bdrms = 3), 
        interval = "confidence",
        level = 0.95)
```

$$Pr(127.29\leq\mu_{Y|X}\leq182.15)=0.95$$

We can now state with 95% confidence that the *population mean house price* of all 1000 square-foot houses with 3 bedrooms is somewhere between \$127,290 and \$182,150. Note that the confidence interval around the mean response is centered at our conditional expectation $(\hat{Y})$ just like all confidence intervals are centered around its estimate.

#### An individual response: a prediction interval {-}

Suppose that instead of building a confidence interval around the conditional average in the population, we want to determine the range within which we are confident to draw a *single* home value. This calculation is almost identical to the mean response above, but with one slight difference.

$$ \hat{Y}_{X} \pm t_{(\frac{\alpha}{2},df=n-k-1)}S_{YX} \sqrt{1+h_i}$$
or

$$ \hat{Y}_{X} - t_{(\frac{\alpha}{2},df=n-k-1)}S_{YX} \sqrt{1+h_i} \leq Y_{X} \leq \hat{Y}_{X} + t_{(\frac{\alpha}{2},df=n-k-1)}S_{YX} \sqrt{1+h_i}$$ 

where

* $\hat{Y}_{X}$ is the expectation of the dependent variable conditional on the desired value of $X_i$.

* $S_{YX}$ is the standard error of the estimate (calculated previously)

* $t_{(\frac{\alpha}{2},df=n-k-1)}$ is the critical t statistic for a given value of $\alpha$ (calculate previously)

* $h_i = \frac{1}{n}+\frac{(X_i - \bar{X})^2}{\sum_{i=1}^n(X_i - \bar{X})^2}$

The only difference is that we replace $\sqrt{h_i}$ with $\sqrt{1+h_i}$. Conceptually, we inserted the one in the formula because we are selecting a *single* home with a specified size and number of bedrooms out of the population. This is very different from building a confidence interval around a population mean, but in R it is simply the change of one word.

```{r}
predict(REG,
        data.frame(sqrft = 1000, bdrms = 3), 
        interval = "prediction",
        level = 0.95)
```

$$Pr(26.40\leq Y_{X} \leq 283.03)=0.95$$

We can now state with 95% confidence that *a single draw of a house price* from the population of all 1000 square-foot houses will be somewhere between \$26,400 and \$283,030. Note that the prediction interval is also centered at our conditional expectation $(\hat{Y})$, but now the interval is much wider than in the previous calculation. This should make sense, because when you are selecting a single home then you have a positive probability of selecting either very cheap homes or very expensive homes. A mean would wash these extreme values out.


